---
layout:     post
title:      "OneHot编码的意义"
subtitle:   ""
date:       2024-09-03 23:00:00
author:     "Simon"
catalog: true
header-img: "img/Earth-2K-Wallpaper.jpg"
tags:
   - 机器学习

---

# What

特征一般有两种：**分类变量（定性特征）与连续变量（定量特征）。**以广告收入增长率为例，如果取值为0-1之间任意数，则此时变量为连续变量。如果把增长率进行分段处理，表示成如下形式：[0,0.3],(0.3,0.6],(0.6,1]，那么此时变量为分类变量。

**特征转换。**对于分类变量，建模时要进行转换，通常直接转换为数字。比如将[0,0.3],(0.3,0.6],(0.6,1]表示为0,1,2。原因主要有两点：

1. 转换后可以提高模型运算效率。
2. 对于一些模型，比如逻辑回归或计算距离时，无法对分类值直接进行计算。

 直接转换为数字，也会带来一些问题：

1. 转换为数字后，默认为连续变量，违背最初设计，影响效率。
2. 转换后的值会影响同一特征在样本中的权重。比如转换为1000和转换为1对模型影响明显不同。

因此，需要更好的编码方式对特征进行转换。

**one-hot编码。**one-hot编码的定义是用N位状态寄存器来对N个状态进行编码。比如上面的例子[0,0.3],(0.3,0.6],(0.6,1]，有3个分类值，因此N为3，对应的one-hot编码可以表示为100,010,001。

# Why

大部分算法是基于向量空间中的度量来进行计算的，为了使非偏序关系的变量取值不具有偏序性，并且到原点是等距的。使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用one-hot编码，会让特征之间的距离计算更加合理。

离散特征进行one-hot编码后，编码后的特征，其实每一维度的特征都可以看做是连续的特征。就可以跟对连续型特征的归一化方法一样，对每一维特征进行归一化。比如归一化到[-1,1]或归一化到均值为0,方差为1。

再贴出某位大佬的解释：**使用one-hot的直接原因是现在多分类cnn网络的输出通常是softmax层，而它的输出是一个概率分布，从而要求输入的标签也以概率分布的形式出现，进而计算交叉熵之类。****one-hot其实就是给出了真实样本的真实概率分布，其中一个样本数据概率为1，其他全为0.。计算损失交叉熵时，直接用1\*log（1/概率），就直接算出了交叉熵，作为损失。**

>  **为什么特征向量要映射到欧式空间？**
>
> 将离散特征通过one-hot编码映射到欧式空间，是因为，在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。

# OneHot编码的优缺点

- 优点：独热编码解决了分类器不好处理属性数据的问题，在一定程度上也起到了扩充特征的作用。它的值只有0和1，不同的类型存储在垂直的空间。
- 缺点：当类别的数量很多时，特征空间会变得非常大，成为一个高维稀疏矩阵。在这种情况下，一般可以用PCA来减少维度。而且one hot encoding+PCA这种组合在实际中也非常有用。

# **什么情况下(不)用独热编码？**

- 用：独热编码用来解决类别型数据的离散值问题，
- 不用：将离散型特征进行one-hot编码的作用，是为了让距离计算更合理，但如果特征是离散的，并且不用one-hot编码就可以很合理的计算出距离，那么就没必要进行one-hot编码。 有些基于树的算法在处理变量时，并不是基于向量空间度量，数值只是个类别符号，即没有偏序关系，所以不用进行独热编码。 Tree Model不太需要one-hot编码： 对于决策树来说，one-hot的本质是增加树的深度。

总的来说，要是one hot encoding的类别数目不太多，建议优先考虑。 

